{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "When working with Parallel Domain's synthetic data, the standard output format is [Dataset Governance Policy (DGP)](https://github.com/TRI-ML/dgp/blob/master/dgp/proto/README.md).\n",
    "In general, the PD SDK can load from any format, as long as custom decoder exists adhering to the `DatasetDecoderProtocol`.\n",
    "Out of the box, PD SDK comes with a pre-configured `DGPDatasetDecoder` which we can leverage to load data.\n",
    "\n",
    "In this tutorial, we are going to load and access a dataset and its scenes.\n",
    "\n",
    "Initially, we need to select the fitting decoder (in this case: `DGPDatasetDecoder`) and then tell it where our dataset is stored. The location can be either a local filesystem path or an s3 address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from paralleldomain.decoding.dgp.decoder import DGPDatasetDecoder\n",
    "from paralleldomain.model.dataset import Dataset  # optional import, just for type reference in this tutorial\n",
    "\n",
    "dataset_path = \"s3://pd-sdk-c6b4d2ea-0301-46c9-8b63-ef20c0d014e9/testset_dgp\"\n",
    "dgp_decoder = DGPDatasetDecoder(dataset_path=dataset_path)\n",
    "\n",
    "dataset: Dataset = dgp_decoder.get_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset Information\n",
    "\n",
    "Now that the dataset information has been loaded, we query a couple of metadata from it:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Metadata:\n",
      "Name: DefaultDatasetName\n",
      "Available Annotation Types:\n",
      "\t<class 'paralleldomain.model.annotation.BoundingBoxes2D'>\n",
      "\t<class 'paralleldomain.model.annotation.BoundingBoxes3D'>\n",
      "\t<class 'paralleldomain.model.annotation.SemanticSegmentation2D'>\n",
      "\t<class 'paralleldomain.model.annotation.SemanticSegmentation3D'>\n",
      "\t<class 'paralleldomain.model.annotation.InstanceSegmentation2D'>\n",
      "\t<class 'paralleldomain.model.annotation.InstanceSegmentation3D'>\n",
      "\t<class 'paralleldomain.model.annotation.Depth'>\n",
      "\t<class 'paralleldomain.model.annotation.Annotation'>\n",
      "\t<class 'paralleldomain.model.annotation.Annotation'>\n",
      "\t<class 'paralleldomain.model.annotation.OpticalFlow'>\n",
      "\t<class 'paralleldomain.model.annotation.Annotation'>\n",
      "Custom Attributes:\n",
      "\torigin: INTERNAL\n",
      "\tname: DefaultDatasetName\n",
      "\tcreator: \n",
      "\tavailable_annotation_types: [0, 1, 2, 3, 4, 5, 6, 10, 7, 8, 9]\n",
      "\tcreation_date: 2021-06-22T15:16:21.317Z\n",
      "\tversion: \n",
      "\tdescription: \n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Metadata:\")\n",
    "print(\"Name:\", dataset.metadata.name)\n",
    "print(\"Available Annotation Types:\", *[f\"\\t{a}\" for a in dataset.available_annotation_types], sep=\"\\n\")\n",
    "print(\"Custom Attributes:\", *[f\"\\t{k}: {v}\" for k,v in dataset.metadata.custom_attributes.items()], sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the property `.available_annotation_types` includes classes from `paralleldomain.model.annotation`. In tutorials around reading annotations from a dataset, these exact classes will be re-used, and allows for a consistent type-check across objects."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Access available Scenes\n",
    "\n",
    "Every dataset consists of scenes. These can contain ordered (usually: by time) or unordered data.\n",
    "In this example, we are looking to receive a list of scene names by type that have been found within the loaded dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found scene pd-sdk_test_set\n",
      "Found unordered scene pd-sdk_test_set\n"
     ]
    }
   ],
   "source": [
    "for sn in dataset.scene_names:\n",
    "    print(f\"Found scene {sn}\")\n",
    "\n",
    "for usn in dataset.unordered_scene_names:\n",
    "    print(f\"Found unordered scene {usn}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Scene\n",
    "\n",
    "After having retrieved all scene names from a dataset, we get the actual `Scene` object and access a couple of properties as well as child objects.\n",
    "Let's start with scene properties:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'PD': { '@type': 'type.googleapis.com/dgp.proto.ParallelDomainSceneMetadata',\n",
      "          'batch_id': 0,\n",
      "          'cloud_cover': 0.10000000149011612,\n",
      "          'fog_intensity': 0.0,\n",
      "          'location': 'SF_6thAndMission_medium',\n",
      "          'rain_intensity': 0.0,\n",
      "          'region_type': 'NORTHERN_CALIFORNIA',\n",
      "          'scene_type': 'URBAN',\n",
      "          'street_lights': 0.0,\n",
      "          'sun_azimuth': 0,\n",
      "          'sun_elevation': 0,\n",
      "          'time_of_day': 'LS_sky_noon_partlyCloudy_1113_HDS024',\n",
      "          'version': 0,\n",
      "          'wetness': 0}}\n"
     ]
    }
   ],
   "source": [
    "from paralleldomain.model.scene import Scene  # optional import, just for type reference in this tutorial\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "\n",
    "selected_scene = dataset.scene_names[0]  # for future\n",
    "scene: Scene = dataset.get_scene(scene_name=selected_scene)\n",
    "\n",
    "# Use prettyprint for nested dictionaries\n",
    "pp = PrettyPrinter(indent=2)\n",
    "pp.pprint(scene.metadata)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scene metadata usually contains any variables that changes with each scene and are not necessarily consistent across a whole data.\n",
    "In many cases these are environment variables like weather, time of day and location.\n",
    "\n",
    "A `Scene` object also includes the information of the available annotation types. These be consistent in most datasets with the ones on the `Dataset` level, but there is the possibility to vary them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ <class 'paralleldomain.model.annotation.BoundingBoxes2D'>,\n",
      "  <class 'paralleldomain.model.annotation.BoundingBoxes3D'>,\n",
      "  <class 'paralleldomain.model.annotation.SemanticSegmentation2D'>,\n",
      "  <class 'paralleldomain.model.annotation.SemanticSegmentation3D'>,\n",
      "  <class 'paralleldomain.model.annotation.InstanceSegmentation2D'>,\n",
      "  <class 'paralleldomain.model.annotation.InstanceSegmentation3D'>,\n",
      "  <class 'paralleldomain.model.annotation.Depth'>,\n",
      "  <class 'paralleldomain.model.annotation.Annotation'>,\n",
      "  <class 'paralleldomain.model.annotation.Annotation'>,\n",
      "  <class 'paralleldomain.model.annotation.OpticalFlow'>,\n",
      "  <class 'paralleldomain.model.annotation.Annotation'>]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(scene.available_annotation_types)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normally, in a scene, we expect to have more than one frame available, especially when we work with sequential data.\n",
    "These can be accessed through their frame IDs. In DGP datasets, these are usually string representations of increasing integers, but they could be also more explicit identifiers for other datasets, e.g., a string representation of a UNIX time or recording vehicle details included.\n",
    "\n",
    "In our example, the frame IDs follow the pattern of integers in string representation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd-sdk_test_set has 10 frames available.\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{scene.name} has {len(scene.frame_ids)} frames available.\")\n",
    "print(scene.frame_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Frame + Sensor\n",
    "\n",
    "### Frames\n",
    "A `Frame` object is like a timestamp-bracket around different sensor data. If we have multiple sensors mounted on our recording vehicle, then the single data recordings are usually grouped into specific timestamps.\n",
    "We can retrieve a `Frame` object and actually see what the \"grouping datetime\" is:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-22 15:16:21.367000+00:00\n"
     ]
    }
   ],
   "source": [
    "frame_0_id = \"0\"\n",
    "frame_0 = scene.get_frame(frame_id=frame_0_id)\n",
    "print(frame_0.date_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Date/Times are presented as Python's std library `datetime` objects. When decoding data, the PD SDK also adds timezone information to these objects.\n",
    "\n",
    "### Sensors\n",
    "\n",
    "As a next step, we want to see what sensor are available within that scene. In general, sensors are divided into `CameraSensor` and `LidarSensor`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameras:\n",
      "camera_front\n",
      "camera_rear\n",
      "virtual_lidar_front_camera_0\n",
      "virtual_lidar_front_camera_1\n",
      "virtual_lidar_front_camera_2\n",
      "virtual_lidar_rear_camera_0\n",
      "virtual_lidar_rear_camera_1\n",
      "virtual_lidar_rear_camera_2\n",
      "\n",
      "\n",
      "LiDARs:\n",
      "lidar_front\n",
      "lidar_rear\n"
     ]
    }
   ],
   "source": [
    "print(\"Cameras:\", *scene.camera_names, sep='\\n')\n",
    "print(\"\\n\")\n",
    "print(\"LiDARs:\", *scene.lidar_names, sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar one how we used this information to get a scene from a dataset, we can use this to get a sensor from a scene."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "camera_0_name = scene.camera_names[0]\n",
    "camera_0 = scene.get_camera_sensor(camera_name=camera_0_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Knowing which frames and sensors are available allows us to now query for the actual sensor data.\n",
    "As described above, a `Frame` is the time-grouping bracket around different sensor recordings. The actual data for a specific sensor assigned to this represented in a `SensorFrame`.\n",
    "This is where sensor data and annotations live.\n",
    "\n",
    "We can either first select a `Frame` and then pick a `Sensor` or the other way around. They will return the same `SensorFrame` instance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both objects are equal: 140422774215200 == 140422774215200\n"
     ]
    }
   ],
   "source": [
    "camera_frame_via_frame = frame_0.get_camera(camera_name=camera_0_name)\n",
    "camera_frame_via_camera = camera_0.get_frame(frame_id=frame_0_id)\n",
    "\n",
    "assert(camera_frame_via_camera is camera_frame_via_camera)\n",
    "print(f\"Both objects are equal: {id(camera_frame_via_frame)} == {id(camera_frame_via_camera)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Sensor Frames\n",
    "\n",
    "Now that we know how to retrieve `SensorFrame` object for specific sensors and timestamps, we can use those to extract the actual sensor data.\n",
    "\n",
    "### Accessing shared properties\n",
    "While there are `CameraSensorFrame` and `LidarSensorFrame` objects with sensor specific data, there are certain properties which are common to any `SensorFrame`.\n",
    "\n",
    "We are going to print the most basic attributes on a `SensorFrame`, using the example of a `CameraSensorFrame` object."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera_front recorded at 2021-06-22 15:16:21.367000+00:00\n",
      "lidar_front recorded at 2021-06-22 15:16:21.367000+00:00\n"
     ]
    }
   ],
   "source": [
    "# Get `CameraSensorFrame` for the first camera on the first frame within the scene.\n",
    "lidar_0_name = scene.lidar_names[0]\n",
    "\n",
    "camera_0_frame_0 = frame_0.get_camera(camera_name=camera_0_name)\n",
    "lidar_0_frame_0 = frame_0.get_lidar(lidar_name=lidar_0_name)\n",
    "\n",
    "print(f\"{camera_0_frame_0.sensor_name} recorded at {camera_0_frame_0.date_time}\")\n",
    "print(f\"{lidar_0_frame_0.sensor_name} recorded at {lidar_0_frame_0.date_time}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Every `SensorFrame` always has information about the sensor pose (where is it in the world coordinate system?) and sensor extrinsic (how is the sensor positioned relative to the ego-vehicle reference coordinate system=).\n",
    "Poses and Extrinsics are represented as instances of the `Transformation` object. It allows storing 6-DoF information and allows for easy combination with each other.\n",
    "In the example below, we are going to calculate the difference between the camera and the lidar sensor. The difference should be the same when using pose or extrinsic."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: [89.31, -45.03, -176.82],t: [-143.43, 151.38, 12.22]  ->  R: [75.19, -44.71, 173.19],t: [-143.12, 151.8, 13.46]\n",
      "R: [0.0, 90.0, -90.0],t: [1.5, 0.0, 1.5]  ->  R: [0.0, 80.0, -90.0],t: [1.0, 0.0, 2.75]\n"
     ]
    }
   ],
   "source": [
    "print(camera_0_frame_0.pose, \" -> \", lidar_0_frame_0.pose)\n",
    "camera_to_lidar_pose = camera_0_frame_0.pose.inverse @ lidar_0_frame_0.pose\n",
    "\n",
    "print(camera_0_frame_0.extrinsic, \" -> \", lidar_0_frame_0.extrinsic)\n",
    "camera_to_lidar_extrinsic = camera_0_frame_0.extrinsic.inverse @ lidar_0_frame_0.extrinsic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the associated homogenous transformation matrix to compare both results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff Pose: R: [10.0, 0.0, -0.0],t: [-0.0, -1.25, -0.5]\n",
      "Diff Extrinsic: R: [10.0, 0.0, -0.0],t: [0.0, -1.25, -0.5]\n",
      "If you see this, the difference are close to equal.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Diff Pose:\", camera_to_lidar_pose)\n",
    "print(\"Diff Extrinsic:\", camera_to_lidar_extrinsic)\n",
    "\n",
    "assert np.all(np.isclose(camera_to_lidar_pose.transformation_matrix, camera_to_lidar_extrinsic.transformation_matrix, atol=1e-05))\n",
    "print(\"If you see this, the difference are close to equal.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the same manner, it is easily possible to calculate the relative location between two sensors. Let's calculate the difference between two camera sensors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: [0.0, 90.0, -90.0],t: [1.5, 0.0, 1.5]  ->  R: [116.57, -90.0, -153.43],t: [-1.5, 0.0, 1.5]\n",
      "Diff Extrinsic:  R: [180.0, 0.0, -180.0],t: [0.0, -0.0, -3.0]\n"
     ]
    }
   ],
   "source": [
    "camera_1_name = scene.camera_names[1]\n",
    "camera_1_frame_0 = frame_0.get_camera(camera_name=camera_1_name)\n",
    "\n",
    "print(camera_0_frame_0.extrinsic, \" -> \", camera_1_frame_0.extrinsic)\n",
    "print(\"Diff Extrinsic: \", camera_0_frame_0.extrinsic.inverse @ camera_1_frame_0.extrinsic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is important to remember that a sensor extrinsic is provided in the ego-vehicles reference coordinate system. For DGP dataset, that is FLU (Front (x), Left (y), Up (z)).\n",
    "So the translation difference between both sensors in ego-vehicle coordinate system is approx x=-3. When calculating the difference between both extrinsics, we will receive though a value of approx. z=-3. That is because we receive the difference in the camera coordinate system (RDF). In this example, we have two cameras (one front, one rear facing) that are perfectly aligned with the ego-vehicle's longitudinal axis x.\n",
    "\n",
    "If we want to have the camera sensor in a FLU coordinate system, we can simply leverage the `CoordinateSystem` class to take of it for us. Objects of that class can also be combined with `Transformation` objects."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: [90.0, -90.0, -180.0],t: [-3.0, -0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from paralleldomain.utilities.coordinate_system import CoordinateSystem\n",
    "\n",
    "\n",
    "extrinsic_diff = (camera_0_frame_0.extrinsic.inverse @ camera_1_frame_0.extrinsic)\n",
    "RDF_to_FLU = (CoordinateSystem(\"RDF\") > CoordinateSystem(\"FLU\"))\n",
    "\n",
    "print( RDF_to_FLU @ extrinsic_diff)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Acessing Annotations\n",
    "\n",
    "While both `CameraSensorFrame` and `LidarSensorFrame` have the property `.available_annotation_types`, the content will most likely be different.\n",
    "There are shared annotation types which are available for both sensor types, but for example 2D Bounding Boxes are something just available for camera data, or point cloud segmentation only for LiDAR data.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ <class 'paralleldomain.model.annotation.BoundingBoxes2D'>,\n",
      "  <class 'paralleldomain.model.annotation.BoundingBoxes3D'>,\n",
      "  <class 'paralleldomain.model.annotation.Annotation'>,\n",
      "  <class 'paralleldomain.model.annotation.SemanticSegmentation2D'>,\n",
      "  <class 'paralleldomain.model.annotation.InstanceSegmentation2D'>,\n",
      "  <class 'paralleldomain.model.annotation.Depth'>,\n",
      "  <class 'paralleldomain.model.annotation.OpticalFlow'>]\n",
      "[ <class 'paralleldomain.model.annotation.BoundingBoxes3D'>,\n",
      "  <class 'paralleldomain.model.annotation.SemanticSegmentation3D'>,\n",
      "  <class 'paralleldomain.model.annotation.InstanceSegmentation3D'>,\n",
      "  <class 'paralleldomain.model.annotation.Annotation'>]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(camera_0_frame_0.available_annotation_types)\n",
    "pp.pprint(lidar_0_frame_0.available_annotation_types)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To actually the annotations into memory and use them for further analysis, we can leverage the `AnnotationTypes` class.\n",
    "In the example below, we are going to load the 2D Bounding Boxes from a camera frame.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class ID: 5, Instance ID: 6\n",
      "Class ID: 5, Instance ID: 15\n",
      "Class ID: 5, Instance ID: 62\n",
      "Class ID: 5, Instance ID: 79\n",
      "Class ID: 5, Instance ID: 109\n",
      "Class ID: 5, Instance ID: 118\n",
      "Class ID: 5, Instance ID: 154\n",
      "Class ID: 5, Instance ID: 177\n",
      "Class ID: 5, Instance ID: 178\n",
      "Class ID: 5, Instance ID: 187\n"
     ]
    }
   ],
   "source": [
    "from paralleldomain.model.annotation import AnnotationTypes\n",
    "from paralleldomain.model.annotation import BoundingBoxes2D  # optional import, just for type reference in this tutorial\n",
    "\n",
    "\n",
    "# Quick check if `BoundingBoxes2D` is an available annotation type. If not, and we do not check for it, we will receive a `ValueError` exception.\n",
    "if AnnotationTypes.BoundingBoxes2D in camera_0_frame_0.available_annotation_types:\n",
    "    boxes2d: BoundingBoxes2D = camera_0_frame_0.get_annotations(annotation_type=AnnotationTypes.BoundingBoxes2D)\n",
    "\n",
    "    for b in boxes2d.boxes[:10]:\n",
    "        print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the LiDAR sensor, we are going to retrieve the 3D Semantic Segmentation of the point cloud and count objects by class ID. Instead of checking explicitly if the annotation type is available, we are going to use try/catch on a `ValueError`. To see if it works, we will try to receive 2D Bounding Boxes from the LiDAR sensor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiDAR Frame doesn't have <class 'paralleldomain.model.annotation.BoundingBoxes2D'> as annotation type available. Original exception below:\n",
      "The annotation type <class 'paralleldomain.model.annotation.BoundingBoxes2D'> is not available in this sensor frame!\n",
      "{ 3: 1906,\n",
      "  5: 89,\n",
      "  8: 3,\n",
      "  11: 47,\n",
      "  12: 6,\n",
      "  21: 9,\n",
      "  22: 8,\n",
      "  24: 1404,\n",
      "  26: 81,\n",
      "  27: 12,\n",
      "  28: 332,\n",
      "  31: 89,\n",
      "  33: 4,\n",
      "  34: 1,\n",
      "  37: 83,\n",
      "  38: 21,\n",
      "  255: 61}\n"
     ]
    }
   ],
   "source": [
    "from paralleldomain.model.annotation import SemanticSegmentation3D  # optional import, just for type reference in this tutorial\n",
    "\n",
    "\n",
    "annotation_type = AnnotationTypes.BoundingBoxes2D\n",
    "\n",
    "try:\n",
    "    boxes2d: BoundingBoxes2D = lidar_0_frame_0.get_annotations(annotation_type=annotation_type)\n",
    "except ValueError as e:\n",
    "    print(f\"LiDAR Frame doesn't have {annotation_type} as annotation type available. Original exception below:\")\n",
    "    print(str(e))\n",
    "\n",
    "\n",
    "# Move on to the actual task:\n",
    "\n",
    "annotation_type = AnnotationTypes.SemanticSegmentation3D\n",
    "\n",
    "count_by_class_id = {}\n",
    "\n",
    "try:\n",
    "    semseg3d: SemanticSegmentation3D = lidar_0_frame_0.get_annotations(annotation_type=annotation_type)\n",
    "    u_class_ids, u_counts = np.unique(semseg3d.class_ids, return_counts=True)\n",
    "    count_by_class_id = {u_class_ids[idx]: u_counts[idx] for idx in range(len(u_class_ids))}\n",
    "    pp.pprint(count_by_class_id)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"LiDAR Frame doesn't have {annotation_type} as annotation type available.\")\n",
    "    print(str(e))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead of showing just class IDs, we can show the actual class labels quite easily. On the `Scene` object we can retrieve the `ClassMap` for each annotation style.\n",
    "Let's get the one for 3D Semantic Segmentation and print the labels for better readability."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 3: 'Building [1906]',\n",
      "  5: 'Car [89]',\n",
      "  8: 'CrossWalk [3]',\n",
      "  11: 'LaneMarking [47]',\n",
      "  12: 'LimitLine [6]',\n",
      "  21: 'ParkingMeter [9]',\n",
      "  22: 'Pedestrian [8]',\n",
      "  24: 'Road [1404]',\n",
      "  26: 'RoadBoundary(Curb) [81]',\n",
      "  27: 'RoadMarking [12]',\n",
      "  28: 'SideWalk [332]',\n",
      "  31: 'Terrain [89]',\n",
      "  33: 'TrafficLight [4]',\n",
      "  34: 'TrafficSign [1]',\n",
      "  37: 'Vegetation [83]',\n",
      "  38: 'VerticalPole [21]',\n",
      "  255: 'Void [61]'}\n"
     ]
    }
   ],
   "source": [
    "from paralleldomain.model.class_mapping import ClassMap  # optional import, just for type reference in this tutorial\n",
    "\n",
    "semseg3d_classmap: ClassMap = scene.get_class_map(annotation_type=AnnotationTypes.SemanticSegmentation3D)\n",
    "\n",
    "count_by_class_label = {k: f\"{semseg3d_classmap[k].name} [{v}]\" for k,v in count_by_class_id.items()}\n",
    "\n",
    "pp.pprint(count_by_class_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}